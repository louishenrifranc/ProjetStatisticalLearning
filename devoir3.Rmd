---
title: "Devoir Statistical Learning 3"
author: "Louis Henri Franc"
date: "1 novembre 2016"
output:
  pdf_document: default
  html_document: default
---

```{r setup}
library("boot")
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
```

# Exercice1 
__a)__ Determinez l'estimateur du maximum de vraisemblance $\hat(\beta))$ de $\beta$ dans le cas général où $\epsilon \sim \mathcal{N}(0,W)$, et $W$ est la matrice des variances et des covariances de $\epsilon$  

$W$ est une matrice de covariance, donc symétrique, positive semi défini.  
On a aussi $\epsilon \sim \mathcal{N}(0,W)$  
Il est possible de faire une décomposition, suivant la formule de Cholesky (matrice hermitienne = matrice symmétrique dans le cas réel)  
Soit $L \in \pmb{\mathbb{R}}^{n,n}$ une matrice triangulaire inférieur.  
$W^{-1} = L^t \cdot L$ car si W est défini positif, $W^{-1}$ l'est aussi.
Si l'on ajoute à droite et à gauche de l'équation $L$, on a,  
$L \cdot Y = L \cdot X \cdot \beta + L \dot \epsilon$  __(1)__  
que l'on peut ré écrire  
$Y^* = X^*\cdot \beta + \epsilon^*$  
avec $Y^* = L \cdot Y$, $X^* = L \cdot X$, et $\epsilon^* = L \cdot \epsilon$  
Les propriétés de $\epsilon^*$ sont:  

##### __Esperance de $\epsilon^*$__
 $E(\epsilon^* | X) = E(L  \epsilon^* | X) = L  E(\epsilon | X) = 0$   

##### __Variance de $\epsilon^*$__
$V(\epsilon^* | X) =  E((L  \epsilon  - 0)  (L  \epsilon - 0)^t)$     
$V(\epsilon^* | X) =  E(L  \epsilon  \epsilon^t L^t)$  
$V(\epsilon^* | X) =  L  E(\epsilon  \epsilon^t) L^t$  
$V(\epsilon^* | X) =  L  V(\epsilon) L^t$  
$V(\epsilon^* | X) =  L  W (L)^t$  
$V(\epsilon^* | X) =  L  (L^tL)^{-1}  L^t$ 
$V(\epsilon^* | X) =  L  L^{-1} (L^t)^{-1}  L^t$  
$V(\epsilon^* | X) = I_d^n$

On s'est donc ramené à la formule classique de régression linéaire où les erreurs sont indépendantes et identiquement distribuées.  
L'estimateur du maximum de vraisemblance, dans le cas de la régression linéaire, est aussi la solution des moindres carrés.  
On a dans le cas général pour $Y = X \beta + \epsilon$  
$\beta_ {opt} = (X^t X)^{-1}X^t Y$  
Ainsi pour l'équation 1, on a   
$\beta_ {opt} = ((X^*)^t X^*)^{-1}(X^*)^t Y^*$  
$\beta_ {opt} = ((LX)^t LX)^{-1}(LX)^t LY$  
$\beta_ {opt} = ((X^t L^t LX)^{-1}(LX)^t LY$  
$\beta_ {opt} = ((X^t L^t LX)^{-1}X^tL^t LY$   
$\beta_ {opt} = (X^tW^{-1}X)^{-1}X^t W^{-1}Y$  

Dans le cas classique de la régression linéaire, l'estimateur $\beta^*$ de beta a une distribution normale asymptotiquement, $\beta^*  \sim \mathcal{N}(0,\sigma^2(X^tX)^{-1})$

__b)__ Déterminer la loi asymptotique de $\hat{\beta}$ et préciser ses paramètres.  

Ici, on a $X^* = L^{-1}X$, donc $\beta_{opt}$ est asymptotiquement distribué normalement tel que $\beta_{opt} \sim \mathcal{N}(0,(X^tWX)^{-1})$

# Exercice 2
#### Question a (Regression logistique)
__1)__ Comparaison des modèles avec la méthode LOOCV  

Pour comparer les méthodes, j'ai utilisé comme valeur de référence, le MSE et le nombre moyen d'erreurs.
```{r}

#
# Code implémenté avant d'utiliser la fonction de la librairie boot
#
#

don<-read.csv2("Capteurs.csv")
don$y= as.factor(don$y)
don$y = (don$y == "C")

MSE1 = MSE2 = MSE3 = NBE1 = NBE2 = NBE3 = 0

for (i in 1:length(don$y)) {
  # On prend  un indice pour notre ensemble de test constitué de un élément.
  testIndex = floor(runif(1,1,length(don$y) + 1))
  
  trainSet = don[-testIndex,]
  testSet = don[testIndex,]
  
  # Nos trois modèles
  md1 = glm( y ~ x1 + x2 , data = trainSet , family=binomial)
  md2 = glm( y ~ x1 + x2 + I(x1*x2), data = trainSet , family=binomial)
  md3 = glm( y ~ x1 + x2 + I(x1*x2)  + I(x1^2) + I(x2^2), data = trainSet , family=binomial)
  
  # Les prédictions des trois modèles.
  predict1 = predict(md1, newdata= as.data.frame(testSet[1:2]),type='response')
  predict2 = predict(md2, newdata= as.data.frame(testSet[1:2]),type='response') 
  predict3 = predict(md3, newdata= as.data.frame(testSet[1:2]),type='response')
  
  # Calcul du MSE
  res = ifelse(don$y[testIndex] == TRUE,1,0)
  MSE1 = MSE1 + (predict1 - res) * (predict1 - res)
  MSE2 = MSE2 + (predict2 - res) * (predict2 - res)
  MSE3 = MSE3 + (predict3 - res) * (predict3 - res)
  
  # Calcul du nombres erreurs
  NBE1 = NBE1 + ifelse(abs(res - predict1) > 0.5,1,0 )
  NBE2 = NBE2 + ifelse(abs(res - predict2) > 0.5,1,0 )
  NBE3 = NBE3 + ifelse(abs(res - predict3) > 0.5,1,0 )
}
# Moyenne des erreurs quadratiques
MSE1 = MSE1 / length(don$y)
MSE2 = MSE2 / length(don$y)
MSE3 = MSE3 / length(don$y)

# taux d'erreur en moyenne
NBE1 = NBE1 / length(don$y)
NBE2 = NBE2 / length(don$y)
NBE3 = NBE3 / length(don$y)

print(paste("MSE modele 1", MSE1))
print(paste("MSE modele 2", MSE2))
print(paste("MSE modele 3", MSE3))

print(paste("Taux d'erreur du modèle 1", NBE1))
print(paste("Taux d'erreur du modèle 2", NBE2))
print(paste("Taux d'erreur du modèle 3", NBE3))
```
On peut aussi la fonction pré-implenté de la librairie boot.
```{r}
md1 = glm( y ~ x1 + x2 , data = don , family=binomial)
md2 = glm( y ~ x1 + x2 + I(x1*x2), data = don , family=binomial)
md3 = glm( y ~ x1 + x2 + I(x1*x2)  + I(x1^2) + I(x2^2), data = don , family=binomial)
print(paste("Moyenne des moindres carrés du modele 1", cv.glm(don,md1)$delta[1]))
print(paste("Moyenne des moindres carrés du modele 2", cv.glm(don,md2)$delta[1]))
print(paste("Moyenne des moindres carrés du modele 3", cv.glm(don,md3)$delta[1]))
```

__2)__ Comparaison des modèles avec la 10 k-fold Cross Validation  

```{r}
print(paste("Moyenne des moindres carrés du modele 1", cv.glm(don,md1,K = 10)$delta[1]))
print(paste("Moyenne des moindres carrés du modele 2", cv.glm(don,md2,K = 10)$delta[1]))
print(paste("Moyenne des moindres carrés du modele 3", cv.glm(don,md3,K = 10)$delta[1]))
```

__3__  Compte tenu des résultats ci dessus, lequel des 3 modèles devrait-on utiliser pour la classification des données du contexte ?   

Selon les deux méthodes de validation, le modèle qui a le MSE le plus petit est le plus simple $Y = \alpha X1 + \beta X2 + \epsilon$. Il semblerait que plus simple le modèle, meilleur est la regression. On pourrait penser que les modèles plus complexes overfittent mais ce n'est pas le cas (Le MSE du training set n'est pas beaucoup plus faible que le MSE du test Set). Prendre le modèle qui a le plus petit MSE, et qui est le plus simple est donc le meilleure choix.

#### Question b (Analyse Discriminante)
__1)__ Pour chacune des deux classes, donner une estimation des paramètres $\pi_j,\mu_j,\Sigma_j,j = C,D$  
On calcule tout d'abord la moyenne et la matrice de covariance/variance pour les deux classes.
```{r}
# Probabilité à priori pour la classe C
priorC = sum(don$y == TRUE) / length(don$y)

# Probabilité à priori pour la classe D
priorD = sum(don$y == FALSE) / length(don$y)

# mu for class C
muC = c(mean(don[which(don$y == TRUE, arr.ind=TRUE),1]), mean(don[which(don$y == TRUE, arr.ind=TRUE),2]))

# mu for class D
muD = c(mean(don[which(don$y == FALSE, arr.ind=TRUE),1]), mean(don[which(don$y == FALSE, arr.ind=TRUE),2]))

library("MASS")
qda = qda(y ~ x1 + x2,data=don)
# divide by (n - 1) for sigma_i_i
varC = matrix(qda$scaling[5:8],nrow =2,byrow = FALSE)
varD = matrix(qda$scaling[1:4],nrow =2,byrow = FALSE)
varC[2,1] = varC[1,2]
varD[2,1] = varD[1,2]

# Pour la classe C, X suit une loi normale de moyenne 
print(muC)
# et de matrice de variance/covariance
print(varC)

# Pour la classe D, X suit une loi normale de moyenne
print(muD)
# et de matrice de variance/covariance
print(varD)
```

__2,3)__ Donner dans chaque cas d'analyse discriminante, l'équatino de la courbe qui sépare les deux classes. Procéder à la classification des 120 observations dans le cas linéaire et dans le cas quadratique, et donner le taux d'erreur dans chaque cas  
```{r}
library("MASS")
don<-read.csv2("Capteurs.csv")

# Linear Discriminant Analysis
lda = lda(y ~ x1 + x2,data=don)
# Calculer le taux d'erreur
prev_lda = predict(lda,don)
prev_lda_y = prev_lda$class
taux_err_lda =  mean(prev_lda_y != don$y)
print(paste("Nombre d'erreurs de LDA",taux_err_lda))
print(paste("Equation qui sépare les deux classes est de la forme X1 *",lda$scaling[1]," + X2",lda$scaling[2]))

# Quadratic Discriminant Analysis
qda = qda(y ~ x1 + x2,data=don)
# Calculer le taux d'erreur
prev_qda = predict(qda,don)
prev_qda_y = prev_qda$class
taux_err_qda =  mean(prev_qda_y != don$y)
print(paste("Nombre d'erreurs de QDA",taux_err_qda))
```

L'équation pour une classe k d'une fonction discriminante quadratique est $\frac{-1}{2} X^t \Sigma_k^{-1} X + X^t \Sigma_k1\mu_k - \frac{1}{2}\mu_k^t\Sigma_k^{-1}\mu_k -\frac{1}{2}\log(|\Sigma_k|) + log\pi_k$
```{r}
discriminant = function(pi, sigma, mu, X) {
  res = -1/2* t(X) %*% solve(sigma) %*% X
  res = res + t(X) %*% sigma %*% mu
  res = res - 1/2 * t(mu) %*%  solve(sigma) %*% mu
  res = res - 1/2 *log(abs(det(sigma)))
  return (res + log(pi))  
}
```

Pour vérifier si la variance a bien été calculé nous allons vérifier si les fonctions de discriminations permettent d'affecter à chaque entrée X un correct Y (soit C, soit D):
```{r}
for(i in 1:20){
 resD = discriminant(priorD, varD, muD, cbind(c(don$x1[i],don$x2[i])))
 resC = discriminant(priorC, varC, muC, cbind(c(don$x1[i],don$x2[i])))
 print(paste(resD, resC, don$y[i]))
}
```

On voit bien ici que lorsque la fonction discriminante de D (première colonne) a une plus grande valeur que la fonction discriminante de C, on a aussi la valeur de Y égale à D, et vice versa (sauf quelques cas ou ce n'est pas le cas car les fonctions discriminantes ont des valeurs très proches).

#### Question c (KNN)
__1)__ Reprendre le k optimal du devoir précédent, et procéder à la classification des 120 observations et donner le taux d'erreur  
```{r}
library("class")
# KNN
# Standardising the data
don_scale = scale(don[,c(1,2)])

# Splitting the data
train = don_scale[0:99,]
test = don_scale[100:120,]
train_X_std = train[,c(1,2)]
test_X_std = test[,c(1,2)]
train_Y = don[0:99,3]
test_Y = don[100:120,3]
knn = knn(train = train_X_std , test = test_X_std , cl = train_Y, k = 5,prob = TRUE)
```


#### Question d (Résume graphique et comparaison)
__1)__ Tracer le nuage de point, et les courbes  séparant les deux classes pour le modèle de régression logistique, l'analyse discriminant linéaire, quadratique, le KNN  
Pour afficher les courbes séparant les classes suivant les différents modèles, nous allons utiliser la fonction contour.
```{r}
# Logistic Regression boundaries
plot(don[1:2],col = ifelse(don$y == "C","red","blue"),xlab = "x1",ylab = "x2")
slope = coef(md1)[2]/(-coef(md1)[3])
intercept = coef(md1)[1]/(-coef(md1)[3]) 
abline(intercept , slope , col="blue")

np = 300
# LDA boundaries
bound.x = seq(from = min(don$x1),to = max(don$x1), length.out = np)
bound.y = seq(from = min(don$x2),to =  max(don$x2),length.out =  np)
bound = expand.grid(x1 = bound.x, x2 = bound.y)
prediction = as.numeric(predict(lda, bound)$class)
contour(x = bound.x, y = bound.y, z = matrix(prediction,nrow = np,ncol = np),levels = c(1, 2), add = TRUE, drawlabels = FALSE,col = "brown",lwd = 2)

# QDA boundaries
prediction1 = as.numeric(predict(qda, bound)$class)
contour(x = bound.x, y = bound.y, z = matrix(prediction1,nrow = np,ncol = np),levels = c(1, 2), add = TRUE, drawlabels = FALSE,col = "red",lwd = 2)

# KNN boundaries
# Pour plotter les données, je n'ai pas scaler les données d'entrainements... ce qui rend l'algorithme plus sensible aux variations des features ayant des grandes valeurs
# Cependant ici les valeurs x1, et x2 ont la moyenne, et la même standart deviation, ce qui ne devrait pas poser trop de problèmes.
knn = knn(train = don[,1:2] , test = bound , cl = don[,3], k = 5,prob = TRUE)
# *sd(bound) + mean(bound),
contour(x = bound.x, y = bound.y, z = matrix(as.numeric(knn), nrow = np,ncol = np),levels = c(1, 2), add = TRUE, drawlabels = FALSE,col = "green",lwd = 2)

legend("bottomright", (c("QDA","Logistic Regression","KNN", "LDA")), lty=1, col=c('red', 'blue', 'green',' brown'),  cex=.75)


```

__2)__ En utilisant la technique de validation croisée "10 fold CV" avec les 120 observations, estimer le taux d'erreur test de chacune des quatre méthodes de classification et conclure  
```{r}
# Determination du taux d'erreur pour les 4 méthodes de classification utilisés précédemment.
# La méthode de validation sera la 10 Fold Cross Validation
nombreFold = length(don$y) / 10
don =  don[sample(nrow(don)),]
folds =  cut(seq(1,nrow(don)) , breaks=nombreFold , labels=FALSE)

NBELDA = 0 # nb erreurs dans LDA
NBEQDA = 0 # nb erreurs dans QDA
NBEKNN = 0 # nb erreurs dans KNN
NBE1 = 0 # nb erreurs dans LR

for (i in 1:length(don$y)) {
  testIndex = floor(runif(1,1,length(don$y) + 1))
  
  trainSet = don[-testIndex,]
  testSet = don[testIndex,]
  
  md1 = glm( y ~ x1 + x2 , data = trainSet , family=binomial)
  predict0 = predict(md1, newdata= as.data.frame(testSet[1:2]),type='response')
  
  predict1 = predict(lda, newdata= as.data.frame(testSet[1:2]),type='response')
  predict2 = predict(qda, newdata= as.data.frame(testSet[1:2]),type='response') 
  knn = knn(train=trainSet[,1:2] ,test= testSet[,1:2] , cl= don$y[-testIndex], k = 5,prob = TRUE)

  NBELDA = NBELDA + sum(predict1$class != testSet$y)
  NBEQDA = NBEQDA + sum(predict2$class != testSet$y)
  NBEKNN = NBEKNN + sum(knn != testSet$y)
  
  res = ifelse(don$y[testIndex] == "D",1,0)
  NBE1 = NBE1 + Reduce("+", ifelse(abs(res - predict0) > 0.5,1,0 ))
}

print(paste("Taux erreur LDA",NBELDA / length(don$y)))
print(paste("Taux erreur QDA",NBEQDA / length(don$y)))
print(paste("Taux erreur KNN",NBEKNN / length(don$y)))
print(paste("Taux erreur de la regression logistique",NBE1 / length(don$y)))
```
La meilleure méthode semble être la méthode des KNN, et le modèle simple de régression logistique. # Pourquoi sachant que ca change des que je modifie le random 


# Exercice 3
__1)__ Donner l'expression d'un estimateur de $\hat{l}$ de _l_ pour n observations  

On travaille sur les données iris. 5 features sont disponibles, le but de la classification sera de prédire l'espèce de la plante.
```{r}
head(iris)
```

On souuhaite estimer le paramètre défini par $l = \mathbb{E}(min(X_1 + X_2, X_1 + X_3 + X_4, X_2 + X_3))$. Pour tout $i = 1,...,150$, $xi = (x_{i1}, x_{i2}, x_{i3}, x_{i4})$ est une observation d'un  vecteur aléatoire $X = (X_1, x_2, X_3, X_4)^t$  

Un estimateur de $l = \mathbb{E}(min{X_1 + X_2, X_1 + X_3 + X_4, X_2 + X_3})$ peut être par exemple $\hat{l} = min(\mathbb{E}(X_1 + X_2), \mathbb{E}(X_1 + X_3 + X_4), \mathbb{E}(X_2 + X_3))$. 

On peut approximer l'espérance par les données que l'on a, on a donc $\hat{l} = min(\sum\limits_{i=1}^{150} \frac{x_{i1} + x_{i2}}{150},\sum\limits_{i=1}^{150} \frac{x_{i1} + x_{i3} + x_{i4}}{150}, \sum\limits_{i=1}^{150} \frac{x_{i2} + x_{i3}}{150})$ 


__2)__
Pour trouver le paramètre ponctuelle du paramètre l, on va utiliser la méthode de bootstrap.  

```{r}
# Estimation ponctuelle du paramètre l
f1 = function(data, index) {
x1 = data[index,1]
x2 = data[index,2]
x3 = data[index,3]
x4 = data[index,4]
return (min((x1 +x2),(x1 +x3 + x4),(x2 + x3))) }
# Evaluation de la précision de l_hat. Obtention de l'écart type 
model = boot(iris, f1, R = 1500)
```

```{r}
# Intervalle de confiance au niveau 95% 
boot.ci(model, type= c("norm"), t0=model$t0[1], t = model$t[,1])
```