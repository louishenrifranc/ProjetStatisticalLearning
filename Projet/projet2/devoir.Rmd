---
title: "Projet"
author: "Louis Henri Franc"
date: "19 novembre 2016"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
```{r, warning=FALSE,message=FALSE,error=FALSE}
# Pour obtenir les mêmes résultats
library("ISLR")
library("pastecs")
library("leaps")
library("glmnet")
library("pls")
library("class")
library("MASS")
set.seed(123)
```

## Introduction au données
Pour le devoir j'ai décidé de travailler sur l'ensemble de donnée __House Sales In King County__. Il est disponible sur cette page (https://www.kaggle.com/harlfoxem/housesalesprediction/kernels). Le but de ce projet sera de __prédire le prix d'une maison en fonction de ces caractéristiques tel que sa superficie, le nombre d'étages, le nombre de chambres.__

```{r cars}
# Importer les données
data <- read.csv("kc_house_data.csv")
```
Regardons ensemble, quelles sont les données disponibles dans ce dataset. 
```{r}
# Afficher quelques lignes
head(data)

# Nombre d'examples
nrow(data)

# Nombre de features
ncol(data)

```

Cette base de donnée est constitué d'un très grand nombre d'examples, ce qui nous permettra d'entrainer plusieurs modèles de prédiction, et de séparer nos données en un ensemble d'entrainement et un ensemble de validation.  

## Sélection des prédicateurs utilisées

Parmi les données, certaines ne nous permettront pas de prédire le prix des maisons, les supprimer est légitime, il s'agit de l'id, la date, le zipcode, la lat, et la longitude. Cela me semblait être des potentiels sources de confusion pour l'entrainement des modèles.  
```{r}
data = data[,-c(1:2, 17:19)]
```
Toutes les données sont numériques. Les variables comme _bathrooms, bedrooms, floors_ seront laissés comme variable numérique, car une différence entre ces valeurs s'interprête relativement bien. Ce n'est pas le cas des variables comme _condition_, _view_ et _grade_. Condition a trois facteurs maximums, tandis que grade en a au moins cinq. Il est donc mieux de les convertir en "dummy variable". Nous allons créer une fonction, que nous appelerons suivant les modèles entrainées.
```{r}
as.dummy_variable = function (data){
  data$floor = factor(data$floor)
  data$view = factor(data$view)
  data$condition = factor(data$condition)
  data$grade = factor(data$grade)
  return(data)
}

```

## Quelques statistiques sur nos données
Afin de comprendre un peu mieux l'ensemble de données, nous allons utiliser la fonction _summary_ et obtenir certaines statistiques sur nos variable explicatives (moyenne, minimal, maximal, variance, médiane)
```{r}
summary(data)
```

## Suppression des données abérrantes
La suppression des données abérrantes autant pour les prédicateurs, que pour les variables explicatives permet, en général, d'améliorer la qualité des modèles. Cela permet par exemple de réduire la variance. Comme nous avons beaucoup d'examples, les supprimer est possible. Nous allons seulement supprimer les outliers des prédicateurs de surface, mais aussi de la variable explicative _prix_. En effet, dans notre base de donnée se trouvent certainement des maisons très cheres, ou très bas prix, et prédire leur prix sera impossible pour tout modèle. Si le modèle est suffisament flexible, il pourra peut être prédire le prix de ces exemples, mais le modèle overfittera certainement.
```{r}
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 4 * IQR(x, na.rm = na.rm)
  y = 
    y = c(which(x < (qnt[1] - H)),which(x > (qnt[2] + H)))
  y
}
num = 0
for(name  in names(data)){
  if(grepl("sqft", name) || name == "price"){
    outliers = remove_outliers(data[,name])
    num = num + length(outliers)
    data = data[-outliers,]
  }
}
# Nombre de données supprimées
print(num)

# Nombre de données restantes
print(nrow(data))
```

Supprimer les données abérrantes va, évidemment, réduire le RSS, MSE calculés de nos données. Mais cela permettra sans doute de converger vers une solution qui généralisera mieux.

## Séparation des ensembles d'entrainements et de tests
Les données vont être séparés en deux groupes qui serviront pour l'entrainement de nos . Pour cela, nous allons utiliser la méthode _sample_
```{r}
smp_size = floor(0.7 * nrow(data))
train_ind = sample(seq_len(nrow(data)), size = smp_size)
train = data[train_ind,]
test = data[-train_ind,]
```


## Simple regression à une variable explicative

Nous allons tout d'abord effectuer des régressions simple à un prédicateur entre le prix et les variables les plus corrélées :
```{r}
cor_matrix = cor(data)
cor_matrix[,"price"]
```
La variable explicative la plus corrélés est _sqft-living. 

#### __Prix en fonction de la surface habitée__
```{r}
lm.fit =lm(price ~ sqft_living, data=train)
summary(lm.fit)
lm.pred = predict(lm.fit, test)
lm.rmse = sqrt(mean((lm.pred - test$price) ^2) )
# Racine carée du MSE du modèle
lm.rmse
```

D'après le R^2 ajusté, la variance dans le prix de la maison s'explique en moitié uniquement par la donnée _sqft-living_. Le RMSE est cependant élevé, ce qui indique que le simple modèle linéaire est probablement incomplet, trop simple, ou incomplet. Nous allons à présent afficher le nuage de point.  
```{r}
pairs(data[, c(1,4)])
```

En regardant la courbe, il semblerait que les résidus augmentent avec la surface habitée. Nous pouvons essayer des modèles en modifiant la variable explicative.
```{r}
lm.fit =lm(price ~  I(sqft_living ^ (2)), data=train)
summary(lm.fit)
lm.pred = predict(lm.fit, test)
sqrt(mean((lm.pred - test$price) ^2) )

```
Remplacer sqft_living par sqrt(sqft_living), ou tout autre polynome (sqft_living^2, ^3) n'améliore pas la régression. 



## Approche par meilleur sous ensembles.
Nous allons utiliser la méthode de _forward selection_ afin de trouver le meilleur modèle de régression, mais aussi le nombre de variables explicatives qui expliquent le modèle. Tout d'abord nous allons étendre le nombre de variables explicatives à 10.  L'estimation de la qualité de la régression sera fait en utilisant le $R^2$ ajusté.

#### Traitement des données.
Afin d'éviter d'avoir des _features_ trop corrélés entre elles, nous allons supprimer toutes les variables qui ont une corrélation supérieur à 0.75. Les méthodes de sélection de sous modèles ont des performances accrues lorsque les variables explicatives ne sont pas trop corrélés.
```{r}
tmp = cor(data)
tmp[lower.tri(tmp)] = 0
diag(tmp) =  0
data1 = data[, !apply(tmp, 2, function(x) any(x > 0.75))]
setdiff(names(data), names(data1))
data = data1
data = as.dummy_variable(data[,-c(5,11)])
train = data[train_ind,]
test = data[-train_ind,]
```
Trois variables ont été supprimés du modèle. Il s'agit de sqft_above, sqftliving15, sqft_lot15. Elles sont en effet très corrélés avec la variable sqft_living...

#### Meilleur sous ensembles
```{r, warnings=FALSE}

# FORWARD STEPWISE SELECTION
regfw.subset = regsubsets(price ~ ., train, nbest = 1, nvmax = 15, method= "forward")
regfwd.summary = summary(regfw.subset)
plot(regfw.subset, scale = "adjr2", main = "R^2 ajusté en fonction des diférents modèles (selection ascendante)")

# BACKWARD STEPWISE SELECTION
regfw.subset = regsubsets(price ~ ., train, nbest = 1, nvmax = 15, method= "backward")
regfwd.summary = summary(regfw.subset)
regfwd.summary
plot(regfw.subset, scale = "adjr2", main = "R^2 ajusté en fonction des diférents modèles (selection descendante)")

```

Nous allons ensuite sélectionner le meilleur des 10 modèles par la méthode de _cross validation_.
```{r, warning=FALSE, message=FALSE, error=FALSE}
# CROSS VALIDATION
folds = sample(rep(1:10, length = nrow(train)))
cv.errors = matrix(NA, 10, 10)

prediction.regsubsets = function(object, newdata, id, ...){
  form = as.formula(object$call[[2]]) #extract object model formula for y ~ x
  mat = model.matrix(form, newdata) #set prediction matrix
  coefi = coef(object, id = id) #obtain matrix of coefficients
  mat[, names(coefi)] %*% coefi #calculate predictions matrix
}

for(k in 1:10){
  best.fit = regsubsets(price ~ .  , data = train[folds != k, ], nvmax = 10, 
                        method = "forward")
  for(i in 1:10){
    pred = prediction.regsubsets(best.fit, train[folds == k, ], id = i)
    
    cv.errors[k, i] = mean((train$price[folds == k] - pred) ^ 2)
  }
}
rmse.cv = sqrt(apply(cv.errors, 2, mean))
plot(rmse.cv, pch = 10, type = "b",xlab = "Nombre de variable explicatives pour les différents meilleures modèles")

# MEILLEUR Modèle sur l'ensemble d'entrainements
which.min(rmse.cv)
```
Le meilleur validé est donc le modèle 10, que nous allons utiliser sur nos données de test.
```{r}
x.test = model.matrix(price ~., data = test)

# Calculer le MSE pour tous les modèles (et voir si le modèle choisi minimise le MSE des données de test)
coefi = coef(regfw.subset, id = 10)
pred = x.test[ , names(coefi)] %*% coefi 
paste("Modele 10, RMSE = ",sqrt(mean((test$price - pred) ^ 2)))
```
Comparé au simple modèle de régression à une variable explicative, le RMSE test a diminué de 30000. Cela s'explique en partie par ce que le modèle est plus flexible.

## Approche par meilleur la régression LASSO
L'avantage de la régression LASSO est qu'elle permet aussi de sélectionner parmi les variables explicatives celles qui  expliquent le mieux la variable explicative. Si le coefficient $\beta$ de la variable explicative tend vers zéro, alors la variable ne peut influencer la prédiction de la variable dépendante.

```{r}
# LASSO REGRESSION MODEL
x = model.matrix(price ~., data=train)
y = train[,1]
fit.lasso = glmnet(x, y, alpha = 1)
plot(fit.lasso, xvar="lambda", label=T)
legend("bottomleft", names(train[-1]), lty=1, cex=.75)

# Valeur optimal de LAMBDA en utilisant la méthode de CROSS VALIDATION
cv.lasso = cv.glmnet(x,y, alpha=1)
plot(cv.lasso)

lambda.min = cv.lasso$lambda.min
print(lambda.min)

fit.lasso = glmnet(x, y,alpha = 1, lambda = lambda.min)
fit.pred = predict(fit.lasso, model.matrix(price~.,data=test))
print(paste("RMSE : ", sqrt(mean((fit.pred - test$price) ^ 2))))

# Coefficient des variables explicatives
coef.lasso = coef(cv.lasso)
coef.lasso
```
Pour un lambda optimal obtenue par la méthode de _cross validation_ (477.1), le coefficient de régression de la variable _yr.renovated_ est tout proche de zéro, ce qui laisse penser qu'il n'influencie pas le prix. 

## Approche composantes principales
L'approche des composantes principales est une autre techniques statistiques permettant de trouver les variables $X_i$ de la régression qui explique le maximum de variance. 
```{r}
pcr.fit = pcr(price ~., data=data, scale=T, validation="CV")
summary(pcr.fit)

validationplot(pcr.fit, val.type = "MSEP")
pcr.fit$validation$adj[9]
```
La valeur optimale M du nombre de variables explicatives est 12. Cependant, on remarque que à partir de 9 variables, le MSE ajusté, ainsi que variance expliquée du modèle ne diminuent presque plus,. Nous allons donc prendre M = 9 pour mesurer notre MSE sur l'ensemble de test.
```{r}
pcr.fit = pcr(price ~ ., data=train, scale=T, validation="CV", ncomp= 9)
pcr.pred = predict(pcr.fit, test, ncomp = 9)
sqrt(mean((pcr.pred - test$price) ^2))
```
Encore une fois le RMSE est au alentour de 160000, ce qui laisse penser que l'on ne peut obtenir meilleur résultat.

## Analyse discriminante linéaire
```{r}
#lda.fit = lda(price ~ ., train)
#plot(lda.fit)

#lda.pred = predict(lda.fit, test)
#srt(mean(lda.pred - test$price) ^2)
```

## Conclusion